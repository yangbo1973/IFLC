{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1fac75f-1781-44c2-94e9-e952634a7c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision.models as models\n",
    "\n",
    "import random\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import data_loader_Animal10N as dataloader\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import gc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f974b76-a9eb-4da5-b565-a288657e66d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch CIFAR Training')\n",
    "parser.add_argument('--batch_size', default=64, type=int, help='train batchsize') \n",
    "parser.add_argument('--lr', '--learning_rate', default=0.02, type=float, help='initial learning rate')\n",
    "parser.add_argument('--num_epochs', default=300, type=int)\n",
    "parser.add_argument('--t_w', default=10, type=int)\n",
    "parser.add_argument('--xi', default=0.04, type=float)\n",
    "parser.add_argument('--eta', default=10., type=float)\n",
    "parser.add_argument('--nc', default=0.2, type=float)\n",
    "parser.add_argument('--nv', default=0.8, type=float)\n",
    "parser.add_argument('--id', default='')\n",
    "parser.add_argument('--seed', default=123)\n",
    "parser.add_argument('--gpuid', default=0, type=int)\n",
    "parser.add_argument('--data_path', default='./data/Animal10N', type=str, help='path to dataset')\n",
    "parser.add_argument('--dataset', default='Animal10N', type=str)\n",
    "args = parser.parse_args(args = ['--data_path', './data/Animal10N',\n",
    "                                 '--dataset', 'Animal10N',\n",
    "                                 '--t_w', '50',\n",
    "                                 '--batch_size','32',\n",
    "                                 '--lr','0.1',\n",
    "                                 '--num_epochs','360',\n",
    "                                 '--xi', '0.04',\n",
    "                                 '--eta','1',\n",
    "                                 '--nc','0.8',\n",
    "                                 '--nv','0.08'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "645caeb7-89ec-45f0-860b-e3395c75bdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(args.gpuid)\n",
    "random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed_all(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c17f27b-2eff-425b-a66b-867171724336",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 50000\n",
    "test_samples = 5000\n",
    "n_class = 10\n",
    "feature_num = 4096\n",
    "t_w = args.t_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "decf99d1-a605-4b67-a2ef-0f64985f472b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch,net,):\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    feature_temp = np.zeros((test_samples, feature_num))\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets, ind) in enumerate(test_loader):\n",
    "            ind = ind.numpy()\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            feature, output = forward_wf(net,inputs)       \n",
    "            _, predicted = torch.max(output, 1)     \n",
    "            \n",
    "            feature_temp[ind] = feature.cpu().detach().numpy()\n",
    "                       \n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).cpu().sum().item()                 \n",
    "    acc = 100.*correct/total\n",
    "    \n",
    "    test_log.write('Epoch:%d   Accuracy:%.2f\\n'%(epoch,acc))\n",
    "    test_log.flush()  \n",
    "    \n",
    "    lossb = relevant_hard_np(feature_temp)\n",
    "    return acc, lossb, feature_temp\n",
    "\n",
    "\n",
    "def linear_rampup(current, warm_up, rampup_length=16):\n",
    "    current = np.clip((current-warm_up) / rampup_length, 0.0, 1.0)\n",
    "    return args.lambda_u*float(current)\n",
    "\n",
    "\n",
    "class NegEntropy(object):\n",
    "    def __call__(self,outputs):\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        return torch.mean(torch.sum(probs.log()*probs, dim=1))\n",
    "    \n",
    "\n",
    "class Orthogonal_loss(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(Orthogonal_loss, self).__init__()\n",
    "        \n",
    "    def forward(self, x, ):\n",
    "        n = x.size(0)\n",
    "        m = x.size(1)\n",
    "\n",
    "        I = torch.eye(m).cuda()\n",
    "        e = x - x.mean(dim=0, keepdims = True)\n",
    "        m_nonz = (e.sum(dim = 0) != 0).sum()\n",
    "        \n",
    "        cov = e.T @ e\n",
    "        \n",
    "        cov2 = cov ** 2\n",
    "        \n",
    "        select_i = torch.argmax(cov2 - cov2 * I, dim = 1)\n",
    "        cov_m = (F.one_hot(select_i, num_classes = m) * cov2).sum()\n",
    "        cov_i = (I * cov).sum()\n",
    "        \n",
    "        result = (cov_m-cov_i) / (m_nonz * n)\n",
    "        return result\n",
    "    \n",
    "def relevant_hard_np(x,):\n",
    "    n = x.shape[0]\n",
    "    nz = x.shape[1]\n",
    "    e = x - x.mean(axis = 0,keepdims = True)\n",
    "\n",
    "    cov = e.T @ e\n",
    "\n",
    "    sigma = (e ** 2).sum(axis = 0, keepdims = True)\n",
    "    r = cov / (sigma.T @ sigma) ** 0.5\n",
    "\n",
    "    r = r ** 2\n",
    "    r[np.isnan(r)] = 0.0\n",
    "\n",
    "    return np.mean(np.max(r - r * np.eye(nz), axis = -1))\n",
    "    \n",
    "def create_model():\n",
    "    model = models.vgg19_bn()\n",
    "    model = model.cuda()\n",
    "    return model     \n",
    "\n",
    "def forward_wf(net, x):\n",
    "    \n",
    "    classifier = net.classifier\n",
    "    first = net.avgpool(net.features(x))\n",
    "    first = torch.reshape(first, (first.shape[0], -1))\n",
    "    feature = classifier[5](classifier[4](classifier[3](classifier[2](classifier[1](classifier[0](first))))))\n",
    "    logits = net.classifier[6](feature)[:,:n_class]\n",
    "    \n",
    "    return feature, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd574e5e-b996-4f8a-ab16-86abf8881e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_log=open('./checkpoint/IFLC_%s_%s'%(\n",
    "    args.dataset,str(datetime.date.today())+'_'+str(time.localtime().tm_hour))+'_stats.txt','w') \n",
    "test_log=open('./checkpoint/IFLC_%s_%s'%(\n",
    "    args.dataset,str(datetime.date.today())+'_'+str(time.localtime().tm_hour))+'_acc.txt','w')     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f167c918-9d7d-4bd4-a553-f46d4970d4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Building net\n"
     ]
    }
   ],
   "source": [
    "\n",
    "loader = dataloader.animal_dataloader(args.dataset,batch_size=args.batch_size,num_workers=5,root_dir=args.data_path,log=stats_log,)\n",
    "\n",
    "print('| Building net')\n",
    "net = create_model()\n",
    "cudnn.benchmark = True\n",
    "\n",
    "opt = optim.SGD(net.parameters(),\n",
    "                lr=args.lr,\n",
    "                momentum=0.9,\n",
    "                # weight_decay=1e-5\n",
    "               )\n",
    "\n",
    "sch = optim.lr_scheduler.MultiStepLR(opt, [150, 250,], gamma = 0.1)\n",
    "\n",
    "CE = nn.CrossEntropyLoss(reduction='none')\n",
    "CEloss = nn.CrossEntropyLoss()\n",
    "\n",
    "loss_ortho = Orthogonal_loss()\n",
    "\n",
    "all_loss = [[],[]] # save the history of losses from two networks\n",
    "\n",
    "traindataset, trainloader = loader.run('warmup')\n",
    "testdataset, test_loader = loader.run('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b876c42d-d14f-4e1c-a060-88dd751c9723",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_Y = np.array(traindataset.noise_label)\n",
    "test_Y = np.array(testdataset.test_label)\n",
    "noisy_Y = np.array(traindataset.noise_label)\n",
    "revised_Y = np.array(traindataset.noise_label)\n",
    "\n",
    "acc_list = []\n",
    "loss_sep_list = [[]]\n",
    "loss_train_list = []\n",
    "Py_temp_list = []\n",
    "\n",
    "score = np.random.rand(samples,)\n",
    "score_uncertainty = np.random.rand(samples,)\n",
    "\n",
    "loss_before_temp = np.random.rand(samples,)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(args.num_epochs):\n",
    "     \n",
    "    net.train()\n",
    "    if epoch < t_w:\n",
    "        _, trainloader = loader.run('warmup')\n",
    "    else:\n",
    "        _, trainloader = loader.run('train')\n",
    "\n",
    "    loss_train = 0\n",
    "    losso_train = 0\n",
    "    acc_train = 0\n",
    "    acc_train_ori = 0\n",
    "    loss_train_ori = 0\n",
    "    Py_temp = np.zeros((samples,),dtype=np.float32)\n",
    "    Pred_temp = np.zeros((samples,),dtype=np.float32)\n",
    "    Probs_temp = np.zeros((samples,n_class),dtype=np.float32)\n",
    "    Logits_temp = np.zeros((samples,n_class),dtype=np.float32)\n",
    "    \n",
    "    feature_temp = np.zeros((samples, feature_num), dtype = np.float32)\n",
    "\n",
    "    loss_after_temp = np.random.rand(samples,)\n",
    "    \n",
    "    loss_normalized = (loss_before_temp - loss_before_temp.min()) / (loss_before_temp.max() - loss_before_temp.min())\n",
    "    mask_rand = np.logical_and(np.random.rand(samples,) >= loss_normalized, np.random.rand(samples,) < args.xi)\n",
    "    \n",
    "    Y_onehot = np.eye(n_class)[revised_Y].astype(np.float32)\n",
    "    Y_onehot_0 = np.eye(n_class)[noisy_Y].astype(np.float32)\n",
    "    \n",
    "    for batch_id, (X_data, targets, ind) in enumerate(trainloader):\n",
    "        ind = ind.numpy()\n",
    "\n",
    "        Y_data = np.array(revised_Y[ind]).astype(np.int64)\n",
    "        Y_data_ori = np.array(train_Y[ind]).astype(np.int64)\n",
    "        temp_X = X_data.cuda()\n",
    "        opt.zero_grad()\n",
    "        Y_GPU = torch.from_numpy(Y_data).cuda()\n",
    "        Y_GPU_ori = torch.from_numpy(Y_data_ori).cuda()\n",
    "        y_onehot = F.one_hot(Y_GPU.view(-1,),num_classes=n_class)\n",
    "        \n",
    "        feature, logits = forward_wf(net, temp_X)\n",
    "\n",
    "        probs = logits.softmax(1)\n",
    "        Py = torch.sum(y_onehot * probs, dim = -1)\n",
    "        Pred = probs.argmax(-1)\n",
    "\n",
    "        loss_o = loss_ortho(feature)\n",
    "        \n",
    "        if epoch >= args.t_w:\n",
    "            mask_ = torch.from_numpy(mask_rand[ind]).cuda()\n",
    "            label_rand = torch.randint(low=0, high = n_class, size=Y_GPU.size()).cuda()\n",
    "            loss_array = CE(logits,torch.where(mask_, label_rand, Y_GPU))\n",
    "            L = loss_array.mean()+loss_o*args.eta\n",
    "        else:\n",
    "            loss_array = CE(logits,Y_GPU)\n",
    "            L = loss_array.mean()\n",
    "            \n",
    "            \n",
    "        Py_temp[ind] = Py.cpu().detach().numpy()\n",
    "        Pred_temp[ind] = Pred.cpu().detach().numpy()\n",
    "\n",
    "        Probs_temp[ind] = probs.cpu().detach().numpy()\n",
    "        Logits_temp[ind] = logits.cpu().detach().numpy()\n",
    "        feature_temp[ind] = feature.cpu().detach().numpy()\n",
    "        \n",
    "        loss_after_temp[ind] = loss_array.cpu().detach().numpy()\n",
    "                \n",
    "        correct = (Pred == Y_GPU).sum().item()\n",
    "        correct_ori = (Pred == Y_GPU_ori).sum().item()\n",
    "\n",
    "        loss_train += loss_array.mean().item()\n",
    "        losso_train += loss_o.item()\n",
    "        acc_train += correct\n",
    "        acc_train_ori += correct_ori\n",
    "        \n",
    "        loss_sep_list[-1].append(loss_array.mean().item())\n",
    "        \n",
    "        L.backward()\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), 1)\n",
    "        opt.step()\n",
    "    sch.step()\n",
    "    \n",
    "    loss_before_temp = loss_after_temp.copy()\n",
    "\n",
    "    loss_train/=(batch_id+1)\n",
    "    losso_train/=(batch_id+1)\n",
    "    acc_train/=samples\n",
    "    acc_train_ori/=samples\n",
    "\n",
    "    print('epoch %d train complete'%epoch)\n",
    "    acc_eval, lossb_eval, feature_val = test(epoch, net)\n",
    "    \n",
    "    feature_temp += np.random.rand(*feature_temp.shape) * 1e-3\n",
    "    feature_val += np.random.rand(*feature_val.shape) * 1e-3\n",
    "    \n",
    "    lossb = relevant_hard_np(feature_temp[:10000])\n",
    "\n",
    "    acc_list.append(acc_eval)\n",
    "    loss_train_list.append(loss_after_temp)\n",
    "    Py_temp_list.append(Py_temp)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(\n",
    "        'train loss:%.4f,train losso:%.4f, train acc:%.4f, train acc ori:%.4f,lossb:%e,\\\n",
    "          eval acc:%.4f, lossb eval:%e, time elapsed:%.4f'\n",
    "    %(loss_train,losso_train, acc_train, acc_train_ori, lossb,\n",
    "     acc_eval, lossb_eval, end_time - start_time))  \n",
    "    \n",
    "    if epoch < t_w:\n",
    "        select = np.zeros((samples,),dtype = np.bool)\n",
    "        score = np.random.rand(samples,)\n",
    "    else:\n",
    "        clean_mask = np.zeros((samples,),dtype = np.bool)\n",
    "        for j_ in range(n_class):\n",
    "            class_mask = noisy_Y == j_\n",
    "            c_n = class_mask.sum()\n",
    "            if c_n > 1:\n",
    "                thres = np.sort(loss_before_temp[class_mask])[int(c_n * args.nc)]\n",
    "                clean_mask[np.logical_and(loss_before_temp < thres, class_mask)] = True\n",
    "        feature_gpu = torch.from_numpy(feature_temp[clean_mask]).cuda()\n",
    "        Y_onehot_gpu = torch.from_numpy(Y_onehot_0[clean_mask],).cuda()\n",
    "        \n",
    "\n",
    "        W = torch.linalg.lstsq(feature_gpu, Y_onehot_gpu).solution\n",
    "\n",
    "        W_cpu = W.cpu().detach().numpy()\n",
    "\n",
    "        f_prob = feature_temp @ W_cpu\n",
    "        f_prob_val = feature_val @ W_cpu\n",
    "        \n",
    "        f_pred = np.argmax(f_prob, axis = -1)\n",
    "        \n",
    "        print('f_prob_l4_val : %.4f'%((np.argmax(f_prob_val, axis = -1) == test_Y).mean()))\n",
    "\n",
    "        score = np.sum((Y_onehot_0 - f_prob)**2, axis = -1)\n",
    "        \n",
    "        r_ = min(0.02 + 0.02 * (epoch - t_w), args.nv)\n",
    "\n",
    "        class_thres = np.sort(score)[int(samples * (1-r_))]\n",
    "        select = score >= class_thres\n",
    "            \n",
    "        revised_Y = np.where(select.ravel(), Pred_temp.ravel(), noisy_Y.ravel()).astype(int)\n",
    "        \n",
    "\n",
    "    is_noise = revised_Y != train_Y\n",
    "    max_noised_class = -999\n",
    "    for j_ in range(n_class):\n",
    "        class_mask = train_Y == j_\n",
    "        noise_n = np.logical_and(class_mask, is_noise).sum()\n",
    "        if noise_n > max_noised_class:\n",
    "            max_noised_class = noise_n\n",
    "    \n",
    "    \n",
    "    print('epoch %d train cleaned, %d samples changed'%(\n",
    "        epoch, np.sum(revised_Y!=noisy_Y)))\n",
    "    Yt_remain_noise = np.sum(is_noise)\n",
    "    print('total remain noise:%.4d, max class noise:%d'%(Yt_remain_noise, max_noised_class))\n",
    "    \n",
    "    loss_sep_list.append([])\n",
    "    \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad10ad7-111b-4862-9a61-1516391be0e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396db517-4619-4a89-9b88-603b8bbc49d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f836728e-e05b-4100-8e27-49a0c8a0a67a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e92b56-1883-4b74-8209-2e9851e76e9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7703d7-82c2-4e3b-8108-564afbeb4f01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
