{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1fac75f-1781-44c2-94e9-e952634a7c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import random\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "from PreResNet_dlh import *\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import dataloader_cifar_dlh0617 as dataloader\n",
    "\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f974b76-a9eb-4da5-b565-a288657e66d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch CIFAR Training')\n",
    "parser.add_argument('--batch_size', default=64, type=int, help='train batchsize') \n",
    "parser.add_argument('--lr', '--learning_rate', default=0.02, type=float, help='initial learning rate')\n",
    "parser.add_argument('--noise_mode',  default='sym')\n",
    "parser.add_argument('--alpha', default=4, type=float, help='parameter for Beta')\n",
    "parser.add_argument('--lambda_u', default=25, type=float, help='weight for unsupervised loss')\n",
    "parser.add_argument('--p_threshold', default=0.5, type=float, help='clean probability threshold')\n",
    "parser.add_argument('--T', default=0.5, type=float, help='sharpening temperature')\n",
    "parser.add_argument('--num_epochs', default=300, type=int)\n",
    "parser.add_argument('--t_w', default=10, type=int)\n",
    "\n",
    "parser.add_argument('--r', default=0.8, type=float, help='noise ratio')\n",
    "parser.add_argument('--id', default='')\n",
    "parser.add_argument('--seed', default=123)\n",
    "parser.add_argument('--gpuid', default=0, type=int)\n",
    "parser.add_argument('--data_path', default='./cifar-10', type=str, help='path to dataset')\n",
    "parser.add_argument('--dataset', default='cifar10', type=str)\n",
    "\n",
    "args = parser.parse_args(args = ['--data_path', 'data/CIFAR100',\n",
    "                                 '--dataset', 'cifar100',\n",
    "                                 '--t_w', '30',\n",
    "                                 '--lambda_u','150',\n",
    "                                 '--lr','0.02',\n",
    "                                 '--noise_mode','asym',\n",
    "                                 '--r','0.4',\n",
    "                                 '--batch_size','64',\n",
    "                                 '--num_epochs','500'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "645caeb7-89ec-45f0-860b-e3395c75bdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(args.gpuid)\n",
    "random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed_all(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8ccdd49-a77c-4c28-ab77-6b8b3e29d587",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 50000\n",
    "test_samples = 10000\n",
    "if args.dataset == 'cifar10':\n",
    "    n_class = 10\n",
    "elif args.dataset == 'cifar100':\n",
    "    n_class = 100\n",
    "else:\n",
    "    raise\n",
    "t_w = args.t_w\n",
    "feature_num = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5923b198-9d49-415d-a557-fbd904715d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch,net,net2,optimizer,labeled_trainloader,unlabeled_trainloader, loss_x):\n",
    "    net.train()\n",
    "    net2.eval() #fix one network and train the other    \n",
    "    feature_temp = np.zeros((samples, feature_num))\n",
    "    \n",
    "    if not isinstance(loss_x, torch.Tensor):\n",
    "        loss_x = torch.from_numpy(loss_x)\n",
    "    loss_x = (loss_x - loss_x.min()) / (loss_x.max() - loss_x.min())\n",
    "    mask_rand = torch.logical_and(torch.rand(len(loss_x),) >= loss_x, torch.rand(len(loss_x),) < 0.0)\n",
    "    \n",
    "    unlabeled_train_iter = iter(unlabeled_trainloader)    \n",
    "    num_iter = (len(labeled_trainloader.dataset)//args.batch_size)+1\n",
    "    for batch_idx, (ind_x, inputs_x, inputs_x2, labels_x, w_x) in enumerate(labeled_trainloader):      \n",
    "        try:\n",
    "            ind_u, inputs_u, inputs_u2 = unlabeled_train_iter.next()\n",
    "        except:\n",
    "            unlabeled_train_iter = iter(unlabeled_trainloader)\n",
    "            ind_u, inputs_u, inputs_u2 = unlabeled_train_iter.next()                 \n",
    "        batch_size = inputs_x.size(0)\n",
    "        \n",
    "        \n",
    "        label_rand = torch.randint(low=0, high = n_class, size=labels_x.size())\n",
    "        labels_x = torch.where(mask_rand[ind_x], label_rand, labels_x)\n",
    "        # Transform label to one-hot\n",
    "        labels_x = torch.zeros(batch_size, n_class).scatter_(1, labels_x.view(-1,1), 1)         \n",
    "        \n",
    "        w_x = torch.minimum(torch.maximum(w_x,torch.tensor(0.)),torch.tensor(1.))\n",
    "        w_x = w_x.view(-1,1).type(torch.FloatTensor) \n",
    "\n",
    "        inputs_x, inputs_x2, labels_x, w_x = inputs_x.cuda(), inputs_x2.cuda(), labels_x.cuda(), w_x.cuda()\n",
    "        inputs_u, inputs_u2 = inputs_u.cuda(), inputs_u2.cuda()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # label co-guessing of unlabeled samples\n",
    "            fe_u11, outputs_u11 = net(inputs_u)\n",
    "            fe_u12, outputs_u12 = net(inputs_u2)\n",
    "            fe_u21, outputs_u21 = net2(inputs_u)\n",
    "            fe_u22, outputs_u22 = net2(inputs_u2)            \n",
    "            \n",
    "            pu = (torch.softmax(outputs_u11, dim=1) + torch.softmax(outputs_u12, dim=1) +\n",
    "                  torch.softmax(outputs_u21, dim=1) + torch.softmax(outputs_u22, dim=1)) / 4       \n",
    "            ptu = pu**(1/args.T) # temparature sharpening\n",
    "            \n",
    "            targets_u = ptu / ptu.sum(dim=1, keepdim=True) # normalize\n",
    "            targets_u = targets_u.detach()       \n",
    "            \n",
    "            # label refinement of labeled samples\n",
    "            fe_x, outputs_x = net(inputs_x)\n",
    "            fe_x2, outputs_x2 = net(inputs_x2)            \n",
    "            \n",
    "            px = (torch.softmax(outputs_x, dim=1) + torch.softmax(outputs_x2, dim=1)) / 2\n",
    "            px = w_x*labels_x + (1-w_x)*px              \n",
    "            ptx = px**(1/args.T) # temparature sharpening \n",
    "                       \n",
    "            targets_x = ptx / ptx.sum(dim=1, keepdim=True) # normalize           \n",
    "            targets_x = targets_x.detach()       \n",
    "        feature_temp[ind_x] = fe_x.cpu().detach().numpy()\n",
    "        # mixmatch\n",
    "        l = np.random.beta(args.alpha, args.alpha)        \n",
    "        l = max(l, 1-l)\n",
    "                \n",
    "        all_inputs = torch.cat([inputs_x, inputs_x2, inputs_u, inputs_u2], dim=0)\n",
    "        all_targets = torch.cat([targets_x, targets_x, targets_u, targets_u], dim=0)\n",
    "\n",
    "        idx = torch.randperm(all_inputs.size(0))\n",
    "\n",
    "        input_a, input_b = all_inputs, all_inputs[idx]\n",
    "        target_a, target_b = all_targets, all_targets[idx]\n",
    "        \n",
    "        mixed_input = l * input_a + (1 - l) * input_b        \n",
    "        mixed_target = l * target_a + (1 - l) * target_b\n",
    "                \n",
    "        feature, logits = net(mixed_input)\n",
    "        logits_x = logits[:batch_size*2]\n",
    "        logits_u = logits[batch_size*2:]        \n",
    "           \n",
    "        Lx, Lu, lamb = criterion(logits_x, mixed_target[:batch_size*2], logits_u, mixed_target[batch_size*2:], epoch+batch_idx/num_iter, t_w)\n",
    "        \n",
    "        # regularization\n",
    "        prior = torch.ones(n_class)/n_class\n",
    "        prior = prior.cuda()        \n",
    "        pred_mean = torch.softmax(logits, dim=1).mean(0)\n",
    "        penalty = torch.sum(prior*torch.log(prior/pred_mean))\n",
    "\n",
    "        loss_o = loss_ortho(fe_x)\n",
    "        \n",
    "        \n",
    "        loss = Lx + lamb * Lu  + penalty + loss_o * 1e1\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        sys.stdout.write('\\r')\n",
    "        sys.stdout.write('%s:%.1f-%s | Epoch [%3d/%3d] Iter[%3d/%3d]\\t Labeled loss: %.2f  Unlabeled loss: %.2f, loss_o : %.2f'\n",
    "                %(args.dataset, args.r, args.noise_mode, epoch, args.num_epochs, batch_idx+1, num_iter, Lx.item(), Lu.item(), loss_o.item()))\n",
    "        sys.stdout.flush()\n",
    "    return feature_temp\n",
    "    \n",
    "\n",
    "def warmup(epoch,net,optimizer,dataloader):\n",
    "    net.train()\n",
    "    num_iter = (len(dataloader.dataset)//dataloader.batch_size)+1\n",
    "    for batch_idx, (ind, inputs, labels, ) in enumerate(dataloader):      \n",
    "        inputs, labels = inputs.cuda(), labels.cuda() \n",
    "        optimizer.zero_grad()\n",
    "        _, outputs = net(inputs)               \n",
    "        loss = CEloss(outputs, labels)      \n",
    "        if args.noise_mode=='asym':  # penalize confident prediction for asymmetric noise\n",
    "            penalty = conf_penalty(outputs)\n",
    "            L = loss + penalty      \n",
    "        elif args.noise_mode=='sym':   \n",
    "            L = loss\n",
    "        L.backward()  \n",
    "        optimizer.step() \n",
    "\n",
    "        sys.stdout.write('\\r')\n",
    "        sys.stdout.write('%s:%.1f-%s | Epoch [%3d/%3d] Iter[%3d/%3d]\\t CE-loss: %.4f'\n",
    "                %(args.dataset, args.r, args.noise_mode, epoch, args.num_epochs, batch_idx+1, num_iter, loss.item()))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "def test(epoch,net1,net2):\n",
    "    net1.eval()\n",
    "    net2.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (ind, inputs, targets) in enumerate(test_loader):\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            fes1, outputs1 = net1(inputs)\n",
    "            fes2, outputs2 = net2(inputs)           \n",
    "            outputs = outputs1+outputs2\n",
    "            _, predicted = torch.max(outputs, 1)            \n",
    "                       \n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).cpu().sum().item()                 \n",
    "    acc = 100.*correct/total\n",
    "    print(\"\\n| Test Epoch #%d\\t Accuracy: %.2f%%\\n\" %(epoch,acc))  \n",
    "    test_log.write('Epoch:%d   Accuracy:%.2f\\n'%(epoch,acc))\n",
    "    test_log.flush()  \n",
    "\n",
    "\n",
    "def eval_train_dlh(model, all_loss, noise_label):    \n",
    "    model.eval()\n",
    "    Y_onehot = np.eye(n_class)[noise_label].astype(np.float32)\n",
    "    feature_temp = np.zeros((samples, feature_num), dtype = np.float32)\n",
    "    \n",
    "    score = torch.zeros(samples)   \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (ind, inputs, targets) in enumerate(eval_loader):\n",
    "            inputs, targets = inputs.cuda(), targets.cuda() \n",
    "            feature, outputs = model(inputs) \n",
    "            loss = CE(outputs, targets)   \n",
    "            score[ind] = loss.cpu()\n",
    "            feature_temp[ind] = feature.cpu().detach().numpy()\n",
    "    score = (score-score.min())/(score.max()-score.min()) \n",
    "    score = score.cpu().detach().numpy()\n",
    "    \n",
    "    clean_mask = np.zeros((samples,),dtype = np.bool)\n",
    "    for j_ in range(n_class):\n",
    "        class_mask = np.array(noise_label) == j_\n",
    "        c_n = class_mask.sum()\n",
    "        if c_n > 1:\n",
    "            thres = np.sort(score[class_mask])[int((c_n-1) * 0.4)]\n",
    "            clean_mask[np.logical_and(score < thres, class_mask)] = True\n",
    "    feature_gpu = torch.from_numpy(feature_temp[clean_mask]).cuda()\n",
    "\n",
    "    Y_onehot_gpu = torch.from_numpy(Y_onehot[clean_mask],).cuda()\n",
    "\n",
    "\n",
    "    W = torch.linalg.lstsq(feature_gpu, Y_onehot_gpu).solution\n",
    "    W_cpu = W.cpu().detach().numpy()\n",
    "\n",
    "    f_prob = feature_temp @ W_cpu   \n",
    "    \n",
    "    losses = np.sum((Y_onehot - f_prob) ** 2, axis = -1)\n",
    "    # losses = (losses - losses.min()) / (losses.max() - losses.min())\n",
    "        \n",
    "    all_loss.append(torch.from_numpy(losses))  \n",
    "    \n",
    "    prob = 1 - losses\n",
    "    \n",
    "    return prob,all_loss, feature_temp, f_prob, score\n",
    "\n",
    "\n",
    "def linear_rampup(current, t_w, rampup_length=16):\n",
    "    current = np.clip((current-t_w) / rampup_length, 0.0, 1.0)\n",
    "    return args.lambda_u*float(current)\n",
    "\n",
    "class SemiLoss(object):\n",
    "    def __call__(self, outputs_x, targets_x, outputs_u, targets_u, epoch, t_w):\n",
    "        probs_u = torch.softmax(outputs_u, dim=1)\n",
    "\n",
    "        Lx = -torch.mean(torch.sum(F.log_softmax(outputs_x, dim=1) * targets_x, dim=1))\n",
    "        Lu = torch.mean((probs_u - targets_u)**2)\n",
    "\n",
    "        return Lx, Lu, linear_rampup(epoch,t_w)\n",
    "\n",
    "class Orthogonal_loss(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(Orthogonal_loss, self).__init__()\n",
    "        \n",
    "    def forward(self, x, ):\n",
    "        n = x.size(0)\n",
    "        m = x.size(1)\n",
    "\n",
    "        I = torch.eye(m).cuda()\n",
    "        e = x - x.mean(dim=0, keepdims = True)\n",
    "        m_nonz = (e.sum(dim = 0) != 0).sum()\n",
    "        \n",
    "        cov = e.T @ e\n",
    "        \n",
    "        cov2 = cov ** 2\n",
    "        \n",
    "        select_i = torch.argmax(cov2 - cov2 * I, dim = 1)\n",
    "        cov_m = (F.one_hot(select_i, num_classes = m) * cov2).sum()\n",
    "        cov_i = (I * cov).sum()\n",
    "        \n",
    "        result = (cov_m-cov_i) / (m_nonz*n)\n",
    "        return result\n",
    "    \n",
    "class NegEntropy(object):\n",
    "    def __call__(self,outputs):\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        return torch.mean(torch.sum(probs.log()*probs, dim=1))\n",
    "\n",
    "def relevant_hard_np(x,):\n",
    "    n = x.shape[0]\n",
    "    nz = x.shape[1]\n",
    "    e = x - x.mean(axis = 0,keepdims = True)\n",
    "\n",
    "    cov = e.T @ e\n",
    "\n",
    "    sigma = (e ** 2).sum(axis = 0, keepdims = True)\n",
    "    r = cov / (sigma.T @ sigma) ** 0.5\n",
    "\n",
    "    r = r ** 2\n",
    "    r[np.isnan(r)] = 0.0\n",
    "    \n",
    "    return np.mean(np.max(r - r * np.eye(nz), axis = -1))\n",
    "    \n",
    "def create_model():\n",
    "    model = ResNet18(num_classes=n_class)\n",
    "    model = model.cuda()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd574e5e-b996-4f8a-ab16-86abf8881e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_log=open('./checkpoint/%s_%.1f_%s_%s'%(args.dataset,args.r,args.noise_mode,\n",
    "                                             str(datetime.date.today())+'_'+str(time.localtime().tm_hour))+'_stats.txt','w') \n",
    "test_log=open('./checkpoint/%s_%.1f_%s_%s'%(args.dataset,args.r,args.noise_mode,\n",
    "                                            str(datetime.date.today())+'_'+str(time.localtime().tm_hour))+'_acc.txt','w')     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f167c918-9d7d-4bd4-a553-f46d4970d4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Building net\n"
     ]
    }
   ],
   "source": [
    "loader = dataloader.cifar_dataloader(args.dataset,r=args.r,noise_mode=args.noise_mode,batch_size=args.batch_size,num_workers=5,\\\n",
    "    root_dir=args.data_path,log=stats_log,noise_file='%s/%.1f_%s.json'%(args.data_path,args.r,args.noise_mode))\n",
    "\n",
    "print('| Building net')\n",
    "net1 = create_model()\n",
    "net2 = create_model()\n",
    "cudnn.benchmark = True\n",
    "\n",
    "criterion = SemiLoss()\n",
    "optimizer1 = optim.SGD(net1.parameters(), lr=args.lr, momentum=0.9, weight_decay=5e-4)\n",
    "optimizer2 = optim.SGD(net2.parameters(), lr=args.lr, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "scheduler1 = optim.lr_scheduler.MultiStepLR(optimizer1, milestones=[300,400], gamma=0.1)\n",
    "scheduler2 = optim.lr_scheduler.MultiStepLR(optimizer2, milestones=[300,400], gamma=0.1)\n",
    "\n",
    "CE = nn.CrossEntropyLoss(reduction='none')\n",
    "CEloss = nn.CrossEntropyLoss()\n",
    "\n",
    "loss_ortho = Orthogonal_loss()\n",
    "\n",
    "if args.noise_mode=='asym':\n",
    "    conf_penalty = NegEntropy()\n",
    "\n",
    "all_loss = [[],[]] # save the history of losses from two networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a693cb9-4880-4f1b-bfa6-2c0b483833f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warmup Net1\n",
      "cifar100:0.4-asym | Epoch [  0/500] Iter[391/391]\t CE-loss: 4.0796\n",
      "Warmup Net2\n",
      "cifar100:0.4-asym | Epoch [  0/500] Iter[391/391]\t CE-loss: 4.0079\n",
      "| Test Epoch #0\t Accuracy: 17.20%\n",
      "\n",
      "Warmup Net1\n",
      "cifar100:0.4-asym | Epoch [  1/500] Iter[391/391]\t CE-loss: 3.7126\n",
      "Warmup Net2\n",
      "cifar100:0.4-asym | Epoch [  1/500] Iter[391/391]\t CE-loss: 3.8704\n",
      "| Test Epoch #1\t Accuracy: 23.12%\n",
      "\n",
      "Warmup Net1\n",
      "cifar100:0.4-asym | Epoch [  2/500] Iter[391/391]\t CE-loss: 3.5102\n",
      "Warmup Net2\n",
      "cifar100:0.4-asym | Epoch [  2/500] Iter[391/391]\t CE-loss: 3.5878\n",
      "| Test Epoch #2\t Accuracy: 29.11%\n",
      "\n",
      "Warmup Net1\n",
      "cifar100:0.4-asym | Epoch [  3/500] Iter[391/391]\t CE-loss: 3.3831\n",
      "Warmup Net2\n",
      "cifar100:0.4-asym | Epoch [  3/500] Iter[391/391]\t CE-loss: 3.2158\n",
      "| Test Epoch #3\t Accuracy: 33.27%\n",
      "\n",
      "Warmup Net1\n",
      "cifar100:0.4-asym | Epoch [  4/500] Iter[391/391]\t CE-loss: 3.1601\n",
      "Warmup Net2\n",
      "cifar100:0.4-asym | Epoch [  4/500] Iter[391/391]\t CE-loss: 3.2395\n",
      "| Test Epoch #4\t Accuracy: 39.59%\n",
      "\n",
      "Warmup Net1\n",
      "cifar100:0.4-asym | Epoch [  5/500] Iter[391/391]\t CE-loss: 3.1697\n",
      "Warmup Net2\n",
      "cifar100:0.4-asym | Epoch [  5/500] Iter[391/391]\t CE-loss: 3.2728\n",
      "| Test Epoch #5\t Accuracy: 42.08%\n",
      "\n",
      "Warmup Net1\n",
      "cifar100:0.4-asym | Epoch [  6/500] Iter[391/391]\t CE-loss: 3.0575\n",
      "Warmup Net2\n",
      "cifar100:0.4-asym | Epoch [  6/500] Iter[391/391]\t CE-loss: 2.9163\n",
      "| Test Epoch #6\t Accuracy: 43.07%\n",
      "\n",
      "Warmup Net1\n",
      "cifar100:0.4-asym | Epoch [  7/500] Iter[391/391]\t CE-loss: 2.9799\n",
      "Warmup Net2\n",
      "cifar100:0.4-asym | Epoch [  7/500] Iter[391/391]\t CE-loss: 2.8914\n",
      "| Test Epoch #7\t Accuracy: 44.55%\n",
      "\n",
      "Warmup Net1\n",
      "cifar100:0.4-asym | Epoch [  8/500] Iter[391/391]\t CE-loss: 2.8558\n",
      "Warmup Net2\n",
      "cifar100:0.4-asym | Epoch [  8/500] Iter[391/391]\t CE-loss: 2.9715\n",
      "| Test Epoch #8\t Accuracy: 49.08%\n",
      "\n",
      "Warmup Net1\n",
      "cifar100:0.4-asym | Epoch [  9/500] Iter[391/391]\t CE-loss: 2.8439\n",
      "Warmup Net2\n",
      "cifar100:0.4-asym | Epoch [  9/500] Iter[391/391]\t CE-loss: 2.6953\n",
      "| Test Epoch #9\t Accuracy: 50.09%\n",
      "\n",
      "Warmup Net1\n",
      "cifar100:0.4-asym | Epoch [ 10/500] Iter[391/391]\t CE-loss: 2.8597\n",
      "Warmup Net2\n",
      "cifar100:0.4-asym | Epoch [ 10/500] Iter[391/391]\t CE-loss: 2.8024\n",
      "| Test Epoch #10\t Accuracy: 49.24%\n",
      "\n",
      "Warmup Net1\n",
      "cifar100:0.4-asym | Epoch [ 11/500] Iter[391/391]\t CE-loss: 2.6971\n",
      "Warmup Net2\n",
      "cifar100:0.4-asym | Epoch [ 11/500] Iter[391/391]\t CE-loss: 2.7171\n",
      "| Test Epoch #11\t Accuracy: 48.50%\n",
      "\n",
      "Warmup Net1\n",
      "cifar100:0.4-asym | Epoch [ 12/500] Iter[391/391]\t CE-loss: 2.6437\n",
      "Warmup Net2\n",
      "cifar100:0.4-asym | Epoch [ 12/500] Iter[391/391]\t CE-loss: 2.7280\n",
      "| Test Epoch #12\t Accuracy: 49.96%\n",
      "\n",
      "Warmup Net1\n",
      "cifar100:0.4-asym | Epoch [ 13/500] Iter[391/391]\t CE-loss: 2.5833\n",
      "Warmup Net2\n",
      "cifar100:0.4-asym | Epoch [ 13/500] Iter[391/391]\t CE-loss: 2.7780\n",
      "| Test Epoch #13\t Accuracy: 51.72%\n",
      "\n",
      "Warmup Net1\n",
      "cifar100:0.4-asym | Epoch [ 14/500] Iter[391/391]\t CE-loss: 2.5507\n",
      "Warmup Net2\n",
      "cifar100:0.4-asym | Epoch [ 14/500] Iter[391/391]\t CE-loss: 2.7295\n",
      "| Test Epoch #14\t Accuracy: 51.95%\n",
      "\n",
      "Warmup Net1\n",
      "cifar100:0.4-asym | Epoch [ 15/500] Iter[391/391]\t CE-loss: 2.5683\n",
      "Warmup Net2\n",
      "cifar100:0.4-asym | Epoch [ 15/500] Iter[391/391]\t CE-loss: 2.4790\n",
      "| Test Epoch #15\t Accuracy: 53.51%\n",
      "\n",
      "Warmup Net1\n",
      "cifar100:0.4-asym | Epoch [ 16/500] Iter[391/391]\t CE-loss: 2.5091\n",
      "Warmup Net2\n",
      "cifar100:0.4-asym | Epoch [ 16/500] Iter[391/391]\t CE-loss: 2.4764\n",
      "| Test Epoch #16\t Accuracy: 54.21%\n",
      "\n",
      "Warmup Net1\n",
      "cifar100:0.4-asym | Epoch [ 17/500] Iter[391/391]\t CE-loss: 2.5840\n",
      "Warmup Net2\n",
      "cifar100:0.4-asym | Epoch [ 17/500] Iter[391/391]\t CE-loss: 2.4260\n",
      "| Test Epoch #17\t Accuracy: 53.90%\n",
      "\n",
      "Warmup Net1\n",
      "cifar100:0.4-asym | Epoch [ 18/500] Iter[391/391]\t CE-loss: 2.4517\n",
      "Warmup Net2\n",
      "cifar100:0.4-asym | Epoch [ 18/500] Iter[391/391]\t CE-loss: 2.5209\n",
      "| Test Epoch #18\t Accuracy: 53.42%\n",
      "\n",
      "Warmup Net1\n",
      "cifar100:0.4-asym | Epoch [ 19/500] Iter[391/391]\t CE-loss: 2.5018\n",
      "Warmup Net2\n",
      "cifar100:0.4-asym | Epoch [ 19/500] Iter[391/391]\t CE-loss: 2.4494\n",
      "| Test Epoch #19\t Accuracy: 54.54%\n",
      "\n",
      "Warmup Net1\n",
      "cifar100:0.4-asym | Epoch [ 20/500] Iter[391/391]\t CE-loss: 2.3179\n",
      "Warmup Net2\n",
      "cifar100:0.4-asym | Epoch [ 20/500] Iter[391/391]\t CE-loss: 2.5457\n",
      "| Test Epoch #20\t Accuracy: 53.44%\n",
      "\n",
      "Warmup Net1\n",
      "cifar100:0.4-asym | Epoch [ 21/500] Iter[391/391]\t CE-loss: 2.2772\n",
      "Warmup Net2\n",
      "cifar100:0.4-asym | Epoch [ 21/500] Iter[391/391]\t CE-loss: 2.3292\n",
      "| Test Epoch #21\t Accuracy: 54.87%\n",
      "\n",
      "Warmup Net1\n",
      "cifar100:0.4-asym | Epoch [ 22/500] Iter[391/391]\t CE-loss: 2.1903\n",
      "Warmup Net2\n",
      "cifar100:0.4-asym | Epoch [ 22/500] Iter[391/391]\t CE-loss: 2.2672\n",
      "| Test Epoch #22\t Accuracy: 54.74%\n",
      "\n",
      "Warmup Net1\n",
      "cifar100:0.4-asym | Epoch [ 23/500] Iter[391/391]\t CE-loss: 2.2215\n",
      "Warmup Net2\n",
      "cifar100:0.4-asym | Epoch [ 23/500] Iter[391/391]\t CE-loss: 2.3486\n",
      "| Test Epoch #23\t Accuracy: 54.10%\n",
      "\n",
      "Warmup Net1\n",
      "cifar100:0.4-asym | Epoch [ 24/500] Iter[391/391]\t CE-loss: 2.2777\n",
      "Warmup Net2\n",
      "cifar100:0.4-asym | Epoch [ 24/500] Iter[391/391]\t CE-loss: 2.2529\n",
      "| Test Epoch #24\t Accuracy: 53.75%\n",
      "\n",
      "Warmup Net1\n",
      "cifar100:0.4-asym | Epoch [ 25/500] Iter[391/391]\t CE-loss: 2.3134\n",
      "Warmup Net2\n",
      "cifar100:0.4-asym | Epoch [ 25/500] Iter[391/391]\t CE-loss: 2.2666\n",
      "| Test Epoch #25\t Accuracy: 52.35%\n",
      "\n",
      "Warmup Net1\n",
      "cifar100:0.4-asym | Epoch [ 26/500] Iter[391/391]\t CE-loss: 2.3430\n",
      "Warmup Net2\n",
      "cifar100:0.4-asym | Epoch [ 26/500] Iter[391/391]\t CE-loss: 2.1863\n",
      "| Test Epoch #26\t Accuracy: 54.38%\n",
      "\n",
      "Warmup Net1\n",
      "cifar100:0.4-asym | Epoch [ 27/500] Iter[391/391]\t CE-loss: 2.2154\n",
      "Warmup Net2\n",
      "cifar100:0.4-asym | Epoch [ 27/500] Iter[391/391]\t CE-loss: 2.1158\n",
      "| Test Epoch #27\t Accuracy: 51.36%\n",
      "\n",
      "Warmup Net1\n",
      "cifar100:0.4-asym | Epoch [ 28/500] Iter[391/391]\t CE-loss: 2.1264\n",
      "Warmup Net2\n",
      "cifar100:0.4-asym | Epoch [ 28/500] Iter[391/391]\t CE-loss: 2.3265\n",
      "| Test Epoch #28\t Accuracy: 52.40%\n",
      "\n",
      "Warmup Net1\n",
      "cifar100:0.4-asym | Epoch [ 29/500] Iter[391/391]\t CE-loss: 2.1871\n",
      "Warmup Net2\n",
      "cifar100:0.4-asym | Epoch [ 29/500] Iter[391/391]\t CE-loss: 2.3043\n",
      "| Test Epoch #29\t Accuracy: 51.15%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_922/1540324515.py:159: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  clean_mask = np.zeros((samples,),dtype = np.bool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Net1\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 30/500] Iter[469/469]\t Labeled loss: 2.03  Unlabeled loss: 0.00, loss_o : 0.30\n",
      "loss_b : 0.21\n",
      "\n",
      "Train Net2\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 30/500] Iter[469/469]\t Labeled loss: 2.68  Unlabeled loss: 0.00, loss_o : 0.39\n",
      "| Test Epoch #30\t Accuracy: 65.04%\n",
      "\n",
      "Train Net1\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 31/500] Iter[469/469]\t Labeled loss: 3.07  Unlabeled loss: 0.00, loss_o : 0.41\n",
      "loss_b : 0.23\n",
      "\n",
      "Train Net2\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 31/500] Iter[469/469]\t Labeled loss: 2.66  Unlabeled loss: 0.00, loss_o : 0.54\n",
      "| Test Epoch #31\t Accuracy: 64.82%\n",
      "\n",
      "Train Net1\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 32/500] Iter[469/469]\t Labeled loss: 3.36  Unlabeled loss: 0.00, loss_o : 0.47\n",
      "loss_b : 0.23\n",
      "\n",
      "Train Net2\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 32/500] Iter[469/469]\t Labeled loss: 2.42  Unlabeled loss: 0.00, loss_o : 0.57\n",
      "| Test Epoch #32\t Accuracy: 65.44%\n",
      "\n",
      "Train Net1\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 33/500] Iter[469/469]\t Labeled loss: 2.98  Unlabeled loss: 0.00, loss_o : 0.51\n",
      "loss_b : 0.22\n",
      "\n",
      "Train Net2\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 33/500] Iter[469/469]\t Labeled loss: 2.66  Unlabeled loss: 0.00, loss_o : 0.52\n",
      "| Test Epoch #33\t Accuracy: 66.02%\n",
      "\n",
      "Train Net1\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 34/500] Iter[469/469]\t Labeled loss: 2.85  Unlabeled loss: 0.00, loss_o : 0.54\n",
      "loss_b : 0.22\n",
      "\n",
      "Train Net2\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 34/500] Iter[469/469]\t Labeled loss: 2.59  Unlabeled loss: 0.00, loss_o : 0.45\n",
      "| Test Epoch #34\t Accuracy: 65.19%\n",
      "\n",
      "Train Net1\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 35/500] Iter[469/469]\t Labeled loss: 2.04  Unlabeled loss: 0.00, loss_o : 0.45\n",
      "loss_b : 0.21\n",
      "\n",
      "Train Net2\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 35/500] Iter[469/469]\t Labeled loss: 2.95  Unlabeled loss: 0.00, loss_o : 0.52\n",
      "| Test Epoch #35\t Accuracy: 66.12%\n",
      "\n",
      "Train Net1\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 36/500] Iter[469/469]\t Labeled loss: 2.87  Unlabeled loss: 0.00, loss_o : 0.62\n",
      "loss_b : 0.21\n",
      "\n",
      "Train Net2\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 36/500] Iter[469/469]\t Labeled loss: 2.45  Unlabeled loss: 0.00, loss_o : 0.52\n",
      "| Test Epoch #36\t Accuracy: 65.80%\n",
      "\n",
      "Train Net1\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 37/500] Iter[469/469]\t Labeled loss: 2.57  Unlabeled loss: 0.00, loss_o : 0.54\n",
      "loss_b : 0.21\n",
      "\n",
      "Train Net2\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 37/500] Iter[469/469]\t Labeled loss: 2.78  Unlabeled loss: 0.00, loss_o : 0.55\n",
      "| Test Epoch #37\t Accuracy: 65.67%\n",
      "\n",
      "Train Net1\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 38/500] Iter[469/469]\t Labeled loss: 2.96  Unlabeled loss: 0.00, loss_o : 0.47\n",
      "loss_b : 0.20\n",
      "\n",
      "Train Net2\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 38/500] Iter[469/469]\t Labeled loss: 2.27  Unlabeled loss: 0.00, loss_o : 0.56\n",
      "| Test Epoch #38\t Accuracy: 65.92%\n",
      "\n",
      "Train Net1\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 39/500] Iter[469/469]\t Labeled loss: 2.76  Unlabeled loss: 0.00, loss_o : 0.52\n",
      "loss_b : 0.20\n",
      "\n",
      "Train Net2\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 39/500] Iter[469/469]\t Labeled loss: 2.66  Unlabeled loss: 0.00, loss_o : 0.55\n",
      "| Test Epoch #39\t Accuracy: 65.80%\n",
      "\n",
      "Train Net1\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 40/500] Iter[469/469]\t Labeled loss: 2.77  Unlabeled loss: 0.00, loss_o : 0.64\n",
      "loss_b : 0.20\n",
      "\n",
      "Train Net2\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 40/500] Iter[469/469]\t Labeled loss: 2.55  Unlabeled loss: 0.00, loss_o : 0.68\n",
      "| Test Epoch #40\t Accuracy: 65.97%\n",
      "\n",
      "Train Net1\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 41/500] Iter[469/469]\t Labeled loss: 2.44  Unlabeled loss: 0.00, loss_o : 0.59\n",
      "loss_b : 0.20\n",
      "\n",
      "Train Net2\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 41/500] Iter[469/469]\t Labeled loss: 2.43  Unlabeled loss: 0.00, loss_o : 0.58\n",
      "| Test Epoch #41\t Accuracy: 66.16%\n",
      "\n",
      "Train Net1\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 42/500] Iter[469/469]\t Labeled loss: 2.65  Unlabeled loss: 0.00, loss_o : 0.53\n",
      "loss_b : 0.19\n",
      "\n",
      "Train Net2\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 42/500] Iter[469/469]\t Labeled loss: 2.64  Unlabeled loss: 0.00, loss_o : 0.46\n",
      "| Test Epoch #42\t Accuracy: 65.58%\n",
      "\n",
      "Train Net1\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 43/500] Iter[469/469]\t Labeled loss: 2.52  Unlabeled loss: 0.00, loss_o : 0.55\n",
      "loss_b : 0.20\n",
      "\n",
      "Train Net2\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 43/500] Iter[469/469]\t Labeled loss: 2.65  Unlabeled loss: 0.00, loss_o : 0.50\n",
      "| Test Epoch #43\t Accuracy: 65.29%\n",
      "\n",
      "Train Net1\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 44/500] Iter[469/469]\t Labeled loss: 2.75  Unlabeled loss: 0.00, loss_o : 0.60\n",
      "loss_b : 0.19\n",
      "\n",
      "Train Net2\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 44/500] Iter[469/469]\t Labeled loss: 2.53  Unlabeled loss: 0.00, loss_o : 0.53\n",
      "| Test Epoch #44\t Accuracy: 65.33%\n",
      "\n",
      "Train Net1\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 45/500] Iter[469/469]\t Labeled loss: 2.43  Unlabeled loss: 0.00, loss_o : 0.61\n",
      "loss_b : 0.19\n",
      "\n",
      "Train Net2\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 45/500] Iter[469/469]\t Labeled loss: 2.51  Unlabeled loss: 0.00, loss_o : 0.52\n",
      "| Test Epoch #45\t Accuracy: 66.42%\n",
      "\n",
      "Train Net1\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 46/500] Iter[469/469]\t Labeled loss: 2.30  Unlabeled loss: 0.00, loss_o : 0.59\n",
      "loss_b : 0.19\n",
      "\n",
      "Train Net2\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 46/500] Iter[469/469]\t Labeled loss: 2.51  Unlabeled loss: 0.00, loss_o : 0.56\n",
      "| Test Epoch #46\t Accuracy: 65.82%\n",
      "\n",
      "Train Net1\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 47/500] Iter[469/469]\t Labeled loss: 2.60  Unlabeled loss: 0.00, loss_o : 0.57\n",
      "loss_b : 0.19\n",
      "\n",
      "Train Net2\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 47/500] Iter[469/469]\t Labeled loss: 2.63  Unlabeled loss: 0.00, loss_o : 0.49\n",
      "| Test Epoch #47\t Accuracy: 66.16%\n",
      "\n",
      "Train Net1\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 48/500] Iter[469/469]\t Labeled loss: 2.24  Unlabeled loss: 0.00, loss_o : 0.54\n",
      "loss_b : 0.19\n",
      "\n",
      "Train Net2\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 48/500] Iter[469/469]\t Labeled loss: 1.59  Unlabeled loss: 0.00, loss_o : 0.52\n",
      "| Test Epoch #48\t Accuracy: 65.96%\n",
      "\n",
      "Train Net1\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 49/500] Iter[469/469]\t Labeled loss: 2.44  Unlabeled loss: 0.00, loss_o : 0.51\n",
      "loss_b : 0.19\n",
      "\n",
      "Train Net2\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 49/500] Iter[469/469]\t Labeled loss: 2.33  Unlabeled loss: 0.00, loss_o : 0.56\n",
      "| Test Epoch #49\t Accuracy: 65.98%\n",
      "\n",
      "Train Net1\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 50/500] Iter[469/469]\t Labeled loss: 1.92  Unlabeled loss: 0.00, loss_o : 0.47\n",
      "loss_b : 0.19\n",
      "\n",
      "Train Net2\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 50/500] Iter[469/469]\t Labeled loss: 1.92  Unlabeled loss: 0.00, loss_o : 0.61\n",
      "| Test Epoch #50\t Accuracy: 65.70%\n",
      "\n",
      "Train Net1\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 51/500] Iter[469/469]\t Labeled loss: 2.56  Unlabeled loss: 0.00, loss_o : 0.57\n",
      "loss_b : 0.18\n",
      "\n",
      "Train Net2\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 51/500] Iter[469/469]\t Labeled loss: 2.65  Unlabeled loss: 0.00, loss_o : 0.55\n",
      "| Test Epoch #51\t Accuracy: 66.08%\n",
      "\n",
      "Train Net1\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 52/500] Iter[469/469]\t Labeled loss: 2.37  Unlabeled loss: 0.00, loss_o : 0.55\n",
      "loss_b : 0.18\n",
      "\n",
      "Train Net2\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 52/500] Iter[469/469]\t Labeled loss: 2.74  Unlabeled loss: 0.00, loss_o : 0.60\n",
      "| Test Epoch #52\t Accuracy: 65.95%\n",
      "\n",
      "Train Net1\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 53/500] Iter[469/469]\t Labeled loss: 2.47  Unlabeled loss: 0.00, loss_o : 0.56\n",
      "loss_b : 0.18\n",
      "\n",
      "Train Net2\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 53/500] Iter[469/469]\t Labeled loss: 2.55  Unlabeled loss: 0.00, loss_o : 0.51\n",
      "| Test Epoch #53\t Accuracy: 66.60%\n",
      "\n",
      "Train Net1\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 54/500] Iter[469/469]\t Labeled loss: 2.39  Unlabeled loss: 0.00, loss_o : 0.51\n",
      "loss_b : 0.18\n",
      "\n",
      "Train Net2\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 54/500] Iter[469/469]\t Labeled loss: 2.77  Unlabeled loss: 0.00, loss_o : 0.55\n",
      "| Test Epoch #54\t Accuracy: 66.43%\n",
      "\n",
      "Train Net1\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 55/500] Iter[469/469]\t Labeled loss: 2.56  Unlabeled loss: 0.00, loss_o : 0.71\n",
      "loss_b : 0.18\n",
      "\n",
      "Train Net2\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 55/500] Iter[469/469]\t Labeled loss: 2.41  Unlabeled loss: 0.00, loss_o : 0.59\n",
      "| Test Epoch #55\t Accuracy: 67.34%\n",
      "\n",
      "Train Net1\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 56/500] Iter[469/469]\t Labeled loss: 2.69  Unlabeled loss: 0.00, loss_o : 0.58\n",
      "loss_b : 0.18\n",
      "\n",
      "Train Net2\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 56/500] Iter[469/469]\t Labeled loss: 2.46  Unlabeled loss: 0.00, loss_o : 0.60\n",
      "| Test Epoch #56\t Accuracy: 66.27%\n",
      "\n",
      "Train Net1\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 57/500] Iter[469/469]\t Labeled loss: 1.97  Unlabeled loss: 0.00, loss_o : 0.52\n",
      "loss_b : 0.18\n",
      "\n",
      "Train Net2\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 57/500] Iter[469/469]\t Labeled loss: 2.57  Unlabeled loss: 0.00, loss_o : 0.55\n",
      "| Test Epoch #57\t Accuracy: 66.69%\n",
      "\n",
      "Train Net1\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 58/500] Iter[469/469]\t Labeled loss: 2.52  Unlabeled loss: 0.00, loss_o : 0.63\n",
      "loss_b : 0.18\n",
      "\n",
      "Train Net2\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 58/500] Iter[469/469]\t Labeled loss: 2.51  Unlabeled loss: 0.00, loss_o : 0.64\n",
      "| Test Epoch #58\t Accuracy: 67.43%\n",
      "\n",
      "Train Net1\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 59/500] Iter[469/469]\t Labeled loss: 2.53  Unlabeled loss: 0.00, loss_o : 0.60\n",
      "loss_b : 0.18\n",
      "\n",
      "Train Net2\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 59/500] Iter[469/469]\t Labeled loss: 2.56  Unlabeled loss: 0.00, loss_o : 0.76\n",
      "| Test Epoch #59\t Accuracy: 66.59%\n",
      "\n",
      "Train Net1\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 60/500] Iter[469/469]\t Labeled loss: 2.13  Unlabeled loss: 0.00, loss_o : 0.54\n",
      "loss_b : 0.18\n",
      "\n",
      "Train Net2\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 60/500] Iter[469/469]\t Labeled loss: 2.49  Unlabeled loss: 0.00, loss_o : 0.57\n",
      "| Test Epoch #60\t Accuracy: 67.37%\n",
      "\n",
      "Train Net1\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 61/500] Iter[469/469]\t Labeled loss: 2.53  Unlabeled loss: 0.00, loss_o : 0.44\n",
      "loss_b : 0.18\n",
      "\n",
      "Train Net2\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 61/500] Iter[469/469]\t Labeled loss: 2.26  Unlabeled loss: 0.00, loss_o : 0.63\n",
      "| Test Epoch #61\t Accuracy: 66.55%\n",
      "\n",
      "Train Net1\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 62/500] Iter[469/469]\t Labeled loss: 2.51  Unlabeled loss: 0.00, loss_o : 0.75\n",
      "loss_b : 0.18\n",
      "\n",
      "Train Net2\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 62/500] Iter[469/469]\t Labeled loss: 2.40  Unlabeled loss: 0.00, loss_o : 0.60\n",
      "| Test Epoch #62\t Accuracy: 66.69%\n",
      "\n",
      "Train Net1\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 63/500] Iter[469/469]\t Labeled loss: 2.78  Unlabeled loss: 0.00, loss_o : 0.57\n",
      "loss_b : 0.18\n",
      "\n",
      "Train Net2\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 63/500] Iter[469/469]\t Labeled loss: 2.40  Unlabeled loss: 0.00, loss_o : 0.54\n",
      "| Test Epoch #63\t Accuracy: 67.28%\n",
      "\n",
      "Train Net1\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 64/500] Iter[469/469]\t Labeled loss: 2.42  Unlabeled loss: 0.00, loss_o : 0.65\n",
      "loss_b : 0.18\n",
      "\n",
      "Train Net2\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 64/500] Iter[469/469]\t Labeled loss: 2.00  Unlabeled loss: 0.00, loss_o : 0.57\n",
      "| Test Epoch #64\t Accuracy: 66.99%\n",
      "\n",
      "Train Net1\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 65/500] Iter[469/469]\t Labeled loss: 2.48  Unlabeled loss: 0.00, loss_o : 0.66\n",
      "loss_b : 0.17\n",
      "\n",
      "Train Net2\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 65/500] Iter[469/469]\t Labeled loss: 2.40  Unlabeled loss: 0.00, loss_o : 0.61\n",
      "| Test Epoch #65\t Accuracy: 67.08%\n",
      "\n",
      "Train Net1\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 66/500] Iter[469/469]\t Labeled loss: 2.33  Unlabeled loss: 0.00, loss_o : 0.53\n",
      "loss_b : 0.17\n",
      "\n",
      "Train Net2\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 66/500] Iter[469/469]\t Labeled loss: 2.46  Unlabeled loss: 0.00, loss_o : 0.75\n",
      "| Test Epoch #66\t Accuracy: 66.77%\n",
      "\n",
      "Train Net1\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 67/500] Iter[469/469]\t Labeled loss: 2.35  Unlabeled loss: 0.00, loss_o : 0.51\n",
      "loss_b : 0.18\n",
      "\n",
      "Train Net2\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 67/500] Iter[469/469]\t Labeled loss: 2.62  Unlabeled loss: 0.00, loss_o : 0.59\n",
      "| Test Epoch #67\t Accuracy: 67.37%\n",
      "\n",
      "Train Net1\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 68/500] Iter[469/469]\t Labeled loss: 2.34  Unlabeled loss: 0.00, loss_o : 0.70\n",
      "loss_b : 0.17\n",
      "\n",
      "Train Net2\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 68/500] Iter[469/469]\t Labeled loss: 2.37  Unlabeled loss: 0.00, loss_o : 0.60\n",
      "| Test Epoch #68\t Accuracy: 67.15%\n",
      "\n",
      "Train Net1\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 69/500] Iter[469/469]\t Labeled loss: 2.56  Unlabeled loss: 0.00, loss_o : 0.61\n",
      "loss_b : 0.17\n",
      "\n",
      "Train Net2\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 69/500] Iter[469/469]\t Labeled loss: 2.43  Unlabeled loss: 0.00, loss_o : 0.69\n",
      "| Test Epoch #69\t Accuracy: 67.31%\n",
      "\n",
      "Train Net1\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 70/500] Iter[469/469]\t Labeled loss: 2.38  Unlabeled loss: 0.00, loss_o : 0.60\n",
      "loss_b : 0.17\n",
      "\n",
      "Train Net2\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 70/500] Iter[469/469]\t Labeled loss: 2.47  Unlabeled loss: 0.00, loss_o : 0.58\n",
      "| Test Epoch #70\t Accuracy: 67.29%\n",
      "\n",
      "Train Net1\n",
      "labeled data has a size of 29998\n",
      "unlabeled data has a size of 20002\n",
      "cifar100:0.4-asym | Epoch [ 71/500] Iter[469/469]\t Labeled loss: 2.29  Unlabeled loss: 0.00, loss_o : 0.52\n",
      "loss_b : 0.17\n",
      "\n",
      "Train Net2\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 71/500] Iter[469/469]\t Labeled loss: 2.11  Unlabeled loss: 0.00, loss_o : 0.50\n",
      "| Test Epoch #71\t Accuracy: 68.14%\n",
      "\n",
      "Train Net1\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 72/500] Iter[469/469]\t Labeled loss: 2.52  Unlabeled loss: 0.00, loss_o : 0.56\n",
      "loss_b : 0.17\n",
      "\n",
      "Train Net2\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 72/500] Iter[469/469]\t Labeled loss: 2.67  Unlabeled loss: 0.00, loss_o : 0.68\n",
      "| Test Epoch #72\t Accuracy: 67.41%\n",
      "\n",
      "Train Net1\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 73/500] Iter[469/469]\t Labeled loss: 2.45  Unlabeled loss: 0.00, loss_o : 0.53\n",
      "loss_b : 0.17\n",
      "\n",
      "Train Net2\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 73/500] Iter[469/469]\t Labeled loss: 2.63  Unlabeled loss: 0.00, loss_o : 0.57\n",
      "| Test Epoch #73\t Accuracy: 68.31%\n",
      "\n",
      "Train Net1\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 74/500] Iter[469/469]\t Labeled loss: 1.84  Unlabeled loss: 0.00, loss_o : 0.60\n",
      "loss_b : 0.17\n",
      "\n",
      "Train Net2\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 74/500] Iter[469/469]\t Labeled loss: 2.24  Unlabeled loss: 0.00, loss_o : 0.59\n",
      "| Test Epoch #74\t Accuracy: 68.15%\n",
      "\n",
      "Train Net1\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 75/500] Iter[469/469]\t Labeled loss: 2.42  Unlabeled loss: 0.00, loss_o : 0.68\n",
      "loss_b : 0.17\n",
      "\n",
      "Train Net2\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 75/500] Iter[469/469]\t Labeled loss: 2.42  Unlabeled loss: 0.00, loss_o : 0.67\n",
      "| Test Epoch #75\t Accuracy: 67.76%\n",
      "\n",
      "Train Net1\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 76/500] Iter[469/469]\t Labeled loss: 2.23  Unlabeled loss: 0.00, loss_o : 0.66\n",
      "loss_b : 0.17\n",
      "\n",
      "Train Net2\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 76/500] Iter[469/469]\t Labeled loss: 2.07  Unlabeled loss: 0.00, loss_o : 0.58\n",
      "| Test Epoch #76\t Accuracy: 68.14%\n",
      "\n",
      "Train Net1\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 77/500] Iter[469/469]\t Labeled loss: 2.52  Unlabeled loss: 0.00, loss_o : 0.57\n",
      "loss_b : 0.17\n",
      "\n",
      "Train Net2\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 77/500] Iter[469/469]\t Labeled loss: 2.37  Unlabeled loss: 0.00, loss_o : 0.52\n",
      "| Test Epoch #77\t Accuracy: 68.12%\n",
      "\n",
      "Train Net1\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 78/500] Iter[469/469]\t Labeled loss: 2.45  Unlabeled loss: 0.00, loss_o : 0.57\n",
      "loss_b : 0.17\n",
      "\n",
      "Train Net2\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 78/500] Iter[469/469]\t Labeled loss: 2.60  Unlabeled loss: 0.00, loss_o : 0.62\n",
      "| Test Epoch #78\t Accuracy: 67.87%\n",
      "\n",
      "Train Net1\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 79/500] Iter[469/469]\t Labeled loss: 2.37  Unlabeled loss: 0.00, loss_o : 0.59\n",
      "loss_b : 0.17\n",
      "\n",
      "Train Net2\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 79/500] Iter[469/469]\t Labeled loss: 2.03  Unlabeled loss: 0.00, loss_o : 0.52\n",
      "| Test Epoch #79\t Accuracy: 67.34%\n",
      "\n",
      "Train Net1\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 80/500] Iter[469/469]\t Labeled loss: 2.53  Unlabeled loss: 0.00, loss_o : 0.52\n",
      "loss_b : 0.17\n",
      "\n",
      "Train Net2\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 80/500] Iter[469/469]\t Labeled loss: 2.46  Unlabeled loss: 0.00, loss_o : 0.57\n",
      "| Test Epoch #80\t Accuracy: 68.26%\n",
      "\n",
      "Train Net1\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 81/500] Iter[469/469]\t Labeled loss: 2.15  Unlabeled loss: 0.00, loss_o : 0.51\n",
      "loss_b : 0.17\n",
      "\n",
      "Train Net2\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 81/500] Iter[469/469]\t Labeled loss: 2.48  Unlabeled loss: 0.00, loss_o : 0.59\n",
      "| Test Epoch #81\t Accuracy: 68.01%\n",
      "\n",
      "Train Net1\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 82/500] Iter[469/469]\t Labeled loss: 2.25  Unlabeled loss: 0.00, loss_o : 0.55\n",
      "loss_b : 0.17\n",
      "\n",
      "Train Net2\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 82/500] Iter[469/469]\t Labeled loss: 2.58  Unlabeled loss: 0.00, loss_o : 0.55\n",
      "| Test Epoch #82\t Accuracy: 67.86%\n",
      "\n",
      "Train Net1\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 83/500] Iter[469/469]\t Labeled loss: 2.57  Unlabeled loss: 0.00, loss_o : 0.70\n",
      "loss_b : 0.17\n",
      "\n",
      "Train Net2\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 83/500] Iter[469/469]\t Labeled loss: 2.64  Unlabeled loss: 0.00, loss_o : 0.58\n",
      "| Test Epoch #83\t Accuracy: 68.41%\n",
      "\n",
      "Train Net1\n",
      "labeled data has a size of 29999\n",
      "unlabeled data has a size of 20001\n",
      "cifar100:0.4-asym | Epoch [ 84/500] Iter[381/469]\t Labeled loss: 2.40  Unlabeled loss: 0.00, loss_o : 0.75"
     ]
    }
   ],
   "source": [
    "for epoch in range(args.num_epochs+1):\n",
    "\n",
    "    test_dataset, test_loader = loader.run('test')\n",
    "    eval_dataset, eval_loader = loader.run('eval_train')   \n",
    "    \n",
    "    if epoch<t_w:       \n",
    "        warmup_dataset, warmup_trainloader = loader.run('warmup')\n",
    "        print('Warmup Net1')\n",
    "        warmup(epoch,net1,optimizer1,warmup_trainloader)    \n",
    "        print('\\nWarmup Net2')\n",
    "        warmup(epoch,net2,optimizer2,warmup_trainloader) \n",
    "   \n",
    "    else:        \n",
    "        start = time.time()\n",
    "        prob1,all_loss[0], feature_temp_1_eval, f_prob_1, score_1 = eval_train_dlh(net1,all_loss[0], eval_dataset.noise_label)   \n",
    "        prob2,all_loss[1], feature_temp_2_eval, f_prob_2, score_2 = eval_train_dlh(net2,all_loss[1], eval_dataset.noise_label)  \n",
    "        \n",
    "                \n",
    "        r_ = 0.4\n",
    "        \n",
    "        thres_1 = np.sort(prob1)[int(samples * r_)]\n",
    "        thres_2 = np.sort(prob2)[int(samples * r_)]\n",
    "        \n",
    "        pred1 = (prob1 > thres_1)\n",
    "        pred2 = (prob2 > thres_2)       \n",
    "\n",
    "        print('Train Net1')\n",
    "        labeled_traindataset, labeled_trainloader, unlabeled_traindataset, unlabeled_trainloader = loader.run('train',pred2,prob2) # co-divide\n",
    "        feature_temp_1 = train(epoch,net1,net2,optimizer1,\n",
    "                                labeled_trainloader, unlabeled_trainloader,\n",
    "                                score_2[labeled_traindataset.predidx]) # train net1  \n",
    "        print('\\nloss_b : %.2f'%relevant_hard_np(feature_temp_1[:10000]))\n",
    "        print('\\nTrain Net2')\n",
    "        labeled_traindataset, labeled_trainloader, unlabeled_traindataset, unlabeled_trainloader = loader.run('train',pred1,prob1) # co-divide\n",
    "        feature_temp_2 = train(epoch,net2,net1,optimizer2,\n",
    "                               labeled_trainloader, unlabeled_trainloader, \n",
    "                               score_1[labeled_traindataset.predidx]) # train net2        \n",
    "        end = time.time()\n",
    "\n",
    "    test(epoch,net1,net2)   \n",
    "    scheduler1.step()\n",
    "    scheduler2.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7d24e8-3d40-49e2-9fba-04b9747e9b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e82b7c-7de2-4f69-80c5-8bb1980e7f45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75560f0-164f-4aa6-878c-b6076130496e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d50f049-e94d-4aaf-9c07-7584a2a6bcb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
